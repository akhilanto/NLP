import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
import re
import numpy as np
import string
import nltk
pd.set_option('display.max_colwidth',100 )
from sklearn.feature_extraction.text import CountVectorizer

data= pd.read_csv("C:\\Users\\akhil\\Desktop\\Research\\Phase 2\\Phase2data.csv", encoding = "latin-1")

stopwords = nltk.corpus.stopwords.words('english')
#ps = nltk.PorterStemmer()
wn = nltk.WordNetLemmatizer()

def clean_textmain(text):
    text = text.lower()
    text ="".join([char for char in text if char not in string.punctuation ])
    text=re.sub(r'\b\w{1,4}\b', '',text)
    result = re.sub(r'[0-9]+','',text)
    tokens = re.split('\W+', result)
    words = [word for word in tokens if word.isalpha()]
    text1=[word for word in words if word not in string.digits]
    #text = [ps.stem(word) for word in text1 if word not in stopwords]
    text = [wn.lemmatize(word) for word in text1 if word not in stopwords]
    return text

datum = pd.DataFrame(data['Title'].apply(clean_textmain))

datum = datum.head(10000)

count_vect_sample = CountVectorizer(ngram_range=(2,2))
X_counts_sample = count_vect_sample.fit_transform(datum.Title.apply(str))
print(X_counts_sample.shape)
#print(count_vect_sample.get_feature_names())

X_counts_df =pd.DataFrame(X_counts_sample.toarray())
#X_counts_df =pd.DataFrame(count_vect_sample.get_feature_names())
X_counts_df.columns = count_vect_sample.get_feature_names()
#print(X_counts_df.head())
#type(X_counts_df)

index1 = np.argsort(X_counts_df.values.sum(axis=0))[::-1][:100]

colnames = X_counts_df.columns[index1]

cleaned =pd.DataFrame(X_counts_df[colnames])

mergeddf = pd.concat([data,cleaned],axis=1)

mergeddfsub = mergeddf[['Citaions'] + ['Year'] + mergeddf.columns[6:].values.tolist()]

dum = pd.DataFrame({'word':[],'elem':[],'cit':[]})
for i in mergeddfsub.columns[2:]:
    z = mergeddfsub.groupby(i).mean()['Citaions']
    for k in range(len(z)):
        #print(i,k,mergeddfsub.groupby(i).mean()['Citations'][k])
        ser = pd.Series([i,k,z[k]],index = ['word','elem','cit'])
        dum = dum.append(ser,ignore_index=True)

dum1=  dum.set_index(['word','elem'])
dum2 = dum1.unstack(level = -1)

dum2.to_csv('dumoutputbigram.csv')

dumy = pd.DataFrame({'word':[],'year':[],'cit':[]})
for i in mergeddfsub.columns[2:]:
    z = mergeddfsub.groupby([i,'Year']).mean()['Citaions']
    z = z[1]
    indd = np.array(z.index).tolist()
    vals = z.values
    for j in range(len(z)): 
       # print(i,indd[j],vals[j])
        ser = pd.Series([i,indd[j],vals[j]],index = ['word','year','cit'])
        dumy = dumy.append(ser,ignore_index=True)

dum3 = dumy.pivot(values='cit',columns='year',index='word')
#dum3.to_csv('dumoutputbigramYearwise.csv')


